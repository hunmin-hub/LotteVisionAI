{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"efficient04_code.ipynb","provenance":[{"file_id":"1qHc-GaQWP19RI0FJMtvwRuHSaOkD6JK_","timestamp":1616162423860},{"file_id":"17QxNH1pNMKCvE4tD8YUHDk-VDkil--e3","timestamp":1615990352992}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GuzsdZd1aNUw","executionInfo":{"status":"ok","timestamp":1616582698319,"user_tz":-540,"elapsed":18508,"user":{"displayName":"김기민","photoUrl":"","userId":"16974130736604264062"}},"outputId":"fcb38f93-b4cc-4987-8828-0cd5917dd4dd"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Buiy1AcM1Wc7","executionInfo":{"status":"ok","timestamp":1616582711955,"user_tz":-540,"elapsed":704,"user":{"displayName":"김기민","photoUrl":"","userId":"16974130736604264062"}},"outputId":"6a978406-1ae4-40a2-8b18-da4471ef5bdc"},"source":["cd /content/drive/MyDrive/workspace/lotte/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/workspace/lotte\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iXDS0G1Jmg6n"},"source":["!unzip ./dataset/LPD_competition.zip -d /content"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sL7vG0ShI0he"},"source":["!pip install efficientnet_pytorch"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vKzNqz-J8Kg6"},"source":["## 라이브러리 import"]},{"cell_type":"code","metadata":{"id":"NwvFl-QA8KIg"},"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms, models\n","import matplotlib.pyplot as plt\n","from efficientnet_pytorch import EfficientNet\n","from tqdm import tqdm\n","import os\n","from tqdm.auto import tqdm\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torchvision import models\n","from glob import glob\n","import cv2\n","from PIL import Image\n","import torch.nn.functional as F\n","from sklearn.model_selection import KFold, StratifiedKFold\n","from torch.optim.lr_scheduler import ReduceLROnPlateau "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lOLmy1bH8R5S"},"source":["## Config Setting"]},{"cell_type":"code","metadata":{"id":"m7hBEsE38Rl6"},"source":["#CONFIG\n","torch.manual_seed(777)\n","BATCH_SIZE=50\n","EPOCHS=30\n","LEARNING_RATE=5e-4\n","#DEVICE\n","print(f'PyTorch Version : [{torch.__version__}]')\n","device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print(f'Device : [{device}]')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oAXOhUln8YtD"},"source":["## Custom Datasets"]},{"cell_type":"code","metadata":{"id":"ZTUb6RoWy98U"},"source":["class LotteDataset(Dataset):\n","  def __init__(self, data_root, train_mode):\n","    super(LotteDataset, self).__init__()\n","    self.train_mode=train_mode\n","\n","    if self.train_mode==False:\n","      self.img_list = glob(os.path.join(data_root, '*.jpg'))\n","      self.img_list.sort(key=lambda x:int(x.split('/')[3][:-4]))\n","    else:\n","      self.img_list = glob(os.path.join(data_root, '*/*.jpg'))\n","      self.train_y=[]\n","      for img_path in self.img_list:\n","        self.train_y.append(int(img_path.split('/')[3]))\n","        \n","    self.len = len(self.img_list)\n","\n","  def __getitem__(self, index):\n","    img_path = self.img_list[index]\n","    if self.train_mode:\n","      label=int(img_path.split('/')[3])\n","    # Image Loading\n","    img = Image.open(img_path)\n","\n","    if self.train_mode:\n","      return img,label\n","    else:\n","      return img\n","\n","  def __len__(self):\n","    return self.len"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6ghn40O-_QBc"},"source":["class MapTransform(Dataset):\n","    def __init__(self, dataset, transform, train_mode):\n","        self.dataset = dataset\n","        self.transform=transform\n","        self.train_mode=train_mode\n","\n","    def __getitem__(self, index):\n","        if self.train_mode:\n","          return self.transform(self.dataset[index][0]), self.dataset[index][1]\n","        else:\n","          return self.transform(self.dataset[index])\n","\n","    def __len__(self):\n","        return len(self.dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tYbVlrwf8aDx"},"source":["train_transforms=transforms.Compose([\n","    transforms.RandomChoice([\n","        transforms.ColorJitter(brightness=(1,1.1)),\n","        transforms.ColorJitter(contrast=0.1), \n","        transforms.ColorJitter(saturation=0.1),\n","    ]),\n","    transforms.RandomChoice([\n","        transforms.RandomAffine(degrees=15, translate=(0.2, 0.2), scale=(0.8, 1.2), shear=10, resample=Image.BILINEAR,fill=255),\n","        transforms.RandomCrop((224,224)),\n","    ]),\n","    transforms.ToTensor(),\n","    transforms.Resize((224, 224)),\n","    transforms.Normalize([0.485, 0.456, 0.406],\n","                         [0.229, 0.224, 0.225])\n","])\n","test_transforms=transforms.Compose([transforms.ToTensor(),\n","                                    transforms.Resize((224,224)),\n","                                    transforms.Normalize([0.485, 0.456, 0.406],\n","                                                          [0.229, 0.224, 0.225])])\n","\n","all_data=LotteDataset('/content/train',train_mode=True)\n","test_data=LotteDataset('/content/test',train_mode=False)\n","\n","trans_test_data=MapTransform(test_data,test_transforms,train_mode=False)\n","test_iter=DataLoader(trans_test_data,batch_size=BATCH_SIZE,shuffle=False,num_workers=2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-az7bt28QuAl"},"source":["### CutMix"]},{"cell_type":"code","metadata":{"id":"FkaUs5YJQolI"},"source":["def rand_bbox(W, H, lam):\n","    cut_rat = torch.sqrt(1.0 - lam)\n","    cut_w = (W * cut_rat).type(torch.long)\n","    cut_h = (H * cut_rat).type(torch.long)\n","    # uniform\n","    cx = torch.randint(W, (1,)).to(device)\n","    cy = torch.randint(H, (1,)).to(device)\n","    x1 = torch.clamp(cx - cut_w // 2, 0, W)\n","    y1 = torch.clamp(cy - cut_h // 2, 0, H)\n","    x2 = torch.clamp(cx + cut_w // 2, 0, W)\n","    y2 = torch.clamp(cy + cut_h // 2, 0, H)\n","    return x1, y1, x2, y2\n","\n","\n","def cutmix_data(x, y, alpha=1.0, p=0.5):\n","    if np.random.random() > p:\n","        return x, y, torch.zeros_like(y), 1.0\n","    W, H = x.size(2), x.size(3)\n","    shuffle = torch.randperm(x.size(0)).to(device)\n","    cutmix_x = x\n","\n","    lam = torch.distributions.beta.Beta(alpha, alpha).sample().to(device)\n","\n","    x1, y1, x2, y2 = rand_bbox(W, H, lam)\n","    cutmix_x[:, :, x1:x2, y1:y2] = x[shuffle, :, x1:x2, y1:y2]\n","    # Adjust lambda to match pixel ratio\n","    lam = 1 - ((x2 - x1) * (y2 - y1) / float(W * H)).item()\n","    y_a, y_b = y, y[shuffle]\n","    return cutmix_x, y_a, y_b, lam"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qlM68asZQ3Pm"},"source":["### Label Smooth"]},{"cell_type":"code","metadata":{"id":"x_Qn7fMcQyZ1"},"source":["def loss_fn(outputs, targets):\n","    if len(targets.shape) == 1:\n","        return F.cross_entropy(outputs, targets)\n","    else:\n","        return torch.mean(torch.sum(-targets * F.log_softmax(outputs, dim=1), dim=1))\n","\n","def label_smooth_loss_fn(outputs, targets, epsilon=0.1):\n","    onehot = F.one_hot(targets, 1000).float().to(device)\n","    targets = (1 - epsilon) * onehot + torch.ones(onehot.shape).to(device) * epsilon / 1000\n","    return loss_fn(outputs, targets)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TW46uRAN-4hS"},"source":["## Model Train"]},{"cell_type":"code","metadata":{"id":"PKsj0EMKDUkl"},"source":["def func_eval(Model,data_iter,loss):\n","    with torch.no_grad():\n","      Model.eval()\n","      n_total, n_correct = 0,0\n","      loss_val_sum=0\n","      print(\"(Train or Validation) Data Testing....\\n\")\n","      for imgs, labels in tqdm(iter(data_iter)):\n","        imgs, labels = imgs.to(device), labels.to(device)\n","        \n","        model_pred=Model(imgs)\n","\n","        loss_out=loss(model_pred,labels)\n","        loss_val_sum+=loss_out\n","\n","        _, y_pred=torch.max(model_pred.data,1)\n","        n_correct+=(y_pred==labels).sum().item()\n","        n_total+=imgs.size(0)\n","      val_accr=(n_correct/n_total)\n","      Model.train()\n","      loss_val_avg=loss_val_sum/len(data_iter)\n","    print(\"Testing Done.\\n\")\n","    return val_accr,loss_val_avg\n","\n","def get_submission(Model,data_iter,epoch,fold):\n","  with torch.no_grad():\n","    Model.eval()\n","    pred_label=[]\n","    print(\"Final Testing....\\n\")\n","    for imgs in tqdm(iter(test_iter)):\n","      model_pred=Model(imgs.to(device))\n","\n","      _, y_pred=torch.max(model_pred.data,1)\n","      pred_label.extend(y_pred.tolist())\n","\n","  Model.train()\n","\n","  submission = pd.read_csv('./dataset/sample.csv', encoding = 'utf-8')\n","  submission['prediction'] = pred_label\n","  submission.to_csv('./submission_Efficient'+str(fold)+'_'+str(epoch)+'.csv', index = False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CWzVi6EJB7BJ"},"source":["folds=StratifiedKFold(n_splits=4,shuffle=True)\n","f = open(\"./efficient04_trainlog.txt\", 'w')\n","\n","for current_fold,(train_idx, vali_idx) in enumerate(folds.split(all_data,all_data.train_y)):\n","  Model = EfficientNet.from_pretrained('efficientnet-b4')\n","  # Model freeze\n","  for m in list(Model.children())[:-2]:\n","    for param in m.parameters():\n","      param.requires_grad=False\n","  Model.eval()\n","  train_data=torch.utils.data.Subset(all_data,train_idx)\n","  vali_data=torch.utils.data.Subset(all_data,vali_idx)\n","\n","  trans_train_data=MapTransform(train_data,train_transforms,train_mode=True)\n","  trans_vali_data=MapTransform(vali_data,test_transforms,train_mode=True)\n","\n","  train_iter=DataLoader(trans_train_data,batch_size=BATCH_SIZE,shuffle=True,num_workers=2)\n","  vali_iter=DataLoader(trans_vali_data,batch_size=BATCH_SIZE,shuffle=True,num_workers=2)\n","\n","  scaler = torch.cuda.amp.GradScaler()\n","  optimizer = optim.Adam(Model.parameters(), lr=LEARNING_RATE)\n","  scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1,threshold_mode='abs',min_lr=1e-8, verbose=True)\n","  loss=label_smooth_loss_fn\n","  print(f'::::::::: Fold : {current_fold} :::::::::::\\n')\n","  f.write(f'::::::::: Fold : {current_fold} :::::::::::\\n')\n","  Model.train()\n","  Model.to(device)\n","  prev_vali_loss=100\n","  best_vali_loss=100\n","  flag=0\n","  over_check=0\n","  for epoch in range(EPOCHS) :\n","    if epoch==3:\n","      for m in Model.children():\n","        for param in m.parameters():\n","          param.requires_grad=True\n","      #Feature 추출 Freeze\n","      for m in list(Model.children())[:-7]:\n","        for param in m.parameters():\n","          param.requires_grad=False\n","    loss_val_sum=0\n","    for imgs, labels in tqdm(iter(train_iter)):\n","      # Cut mix P=0.5\n","      imgs, labels = imgs.to(device), labels.to(device)\n","      imgs, labels_a, labels_b, lam = cutmix_data(imgs, labels)\n","\n","      # optimizer.zero_grad()\n","      for param in Model.parameters():\n","        param.grad = None\n","      model_pred=Model(imgs)\n","      loss_out = lam * loss(model_pred, labels_a) + (1 - lam) * loss(model_pred, labels_b)\n","\n","      scaler.scale(loss_out).backward()\n","      scaler.step(optimizer)\n","      scaler.update()\n","\n","      loss_val_sum+=loss_out\n","\n","    loss_val_avg=loss_val_sum/len(train_iter)\n","    vali_accr,vali_loss=func_eval(Model,vali_iter,loss_fn)\n","    if epoch>19:\n","      get_submission(Model,test_iter,epoch,current_fold)\n","      print(\"epoch:[%d] train loss:[%.5f] vali loss:[%.5f] vali_accr:[%.5f]\\n\"%(epoch,loss_val_avg,vali_loss,vali_accr))\n","      f.write(\"epoch:[%d] train loss:[%.5f] vali loss:[%.5f] vali_accr:[%.5f]\\n\"%(epoch,loss_val_avg,vali_loss,vali_accr))\n","      print(\"Model Save....\\n\")\n","      torch.save({'model_state_dict': Model.state_dict(),\n","                'optimizer_state_dict': optimizer.state_dict()}, './checkpoint_efficient04/'+str(current_fold)+'_efficient04_epoch_'+str(epoch)+'.tar')\n","    else:\n","      print(\"epoch:[%d] train loss:[%.5f] vali loss:[%.5f] vali_accr:[%.5f]\\n\"%(epoch,loss_val_avg,vali_loss,vali_accr))\n","      f.write(\"epoch:[%d] train loss:[%.5f] vali loss:[%.5f] vali_accr:[%.5f]\\n\"%(epoch,loss_val_avg,vali_loss,vali_accr))\n","    scheduler.step(vali_loss) # LR Scheduler\n","\n","    if prev_vali_loss<vali_loss: #Stop Train\n","      flag+=1\n","      if flag==10 : \n","        print(\"Stop Training...\\n\")\n","        break\n","    if best_vali_loss>vali_loss:\n","      flag=0\n","      best_vali_loss=vali_loss\n","    prev_vali_loss=vali_loss\n","f.close()"],"execution_count":null,"outputs":[]}]}